# -*- coding: utf-8 -*-
"""NLP_TextAnalysis_10Q.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Usm4m72Ops7o5k2nTSgDQVE-Rck5ljBJ
"""

import wrds
import pandas as pd
import numpy as np
# First Step: ciks, features, daily returns of S&P500 stocks
# Connect to WRDS
conn = wrds.Connection()
# read the provided
file_path = 'sp500_composition.xlsx'
sp500_df = pd.read_excel(file_path)

"""## Phase One Merge ciks and Returns"""

# Match Daily Returns of Stocks For SP500
# msp500list stocks that is currently or ones in the index
# dsf is the daily price of the stocks
# Notice that some stocks may be exclude from the sp500 list
sp500 = conn.raw_sql("""
                        select a.*, b.date, b.ret
                        from crsp.msp500list as a,
                        crsp.dsf as b
                        where a.permno=b.permno
                        and b.date >= a.start and b.date<= a.ending
                        and b.date>='08/01/2017'
                        order by date;
                        """, date_cols=['start', 'ending', 'date'])
# Daily Price
dse = conn.raw_sql("""
                        select comnam, ncusip, namedt, nameendt,
                        permno, shrcd, exchcd, hsiccd, ticker
                        from crsp.msenames
                        """, date_cols=['namedt'])

# Merge with SP500 data
sp500_full = pd.merge(sp500, dse, how = 'left', on = 'permno')
# Append CIK Following online tutorial in WRDS

sp500_full = sp500_full.drop_duplicates()

ccm=conn.raw_sql("""
                  select gvkey, liid as iid, lpermno as permno,
                  linktype, linkprim, linkdt, linkenddt
                  from crsp.ccmxpf_linktable
                  where substr(linktype,1,1)='L'
                  and (linkprim ='C' or linkprim='P')
                  """, date_cols=['linkdt', 'linkenddt'])
sp500ccm = pd.merge(sp500_full, ccm, how='left', on=['permno'])

sp500ccm = sp500ccm.drop_duplicates()

sp500ccm = sp500ccm[['date', 'permno', 'comnam', 'ncusip',
                     'shrcd', 'exchcd', 'ticker',
                     'gvkey', 'iid', 'start', 'ending', 'ret']]

### Add CIKs and Link with SEC Index Files using CIK

names = conn.raw_sql(""" select gvkey, cik, sic, naics, gind, gsubind from comp.names """)

# Merge sp500 constituents table with names table
sp500cik = pd.merge(sp500ccm, names, on='gvkey',  how='left')
sp500cik.head()

sp500 = sp500cik[['date', 'comnam', 'ticker', 'cik', 'ret']]

sp500 = sp500.sort_values(by=['ticker', 'cik', 'date'])
sp500

sp500 = sp500.drop_duplicates(subset=['ticker', 'cik', 'date'], keep='last')
sp500 = sp500[sp500['ticker'].notna()]
sp500['cik'] = sp500['cik'].fillna(0).astype(int)
sp500['cik'] = sp500['cik'].astype(int)
sp500

"""As we are working on the permno code problem, an issue may arise if there exist some companies that share the same permno but have never been part of the S&P 500. We need to filter these companies out from the S&P 500 series."""

file_path = 'sp500_composition.xlsx'
sp500_comp = pd.read_excel(file_path)

sp500_tick = list(set(sp500_comp['Ticker']))
sp500_filtered = sp500[sp500['ticker'].isin(sp500_tick)]
sp500_filtered['info'] = sp500_filtered['ticker'] + '_' + np.where(sp500_filtered['cik'] == 0, '', sp500_filtered['cik'].astype(str))

pivot_df = sp500_filtered.pivot_table(index='date', columns='info', values='ret', aggfunc='first')

unique_columns = {}
for col in pivot_df.columns:
    base_name = col.split('_')[0]
    if base_name not in unique_columns or len(col) > len(unique_columns[base_name]):
        unique_columns[base_name] = col

pivot_df = pivot_df[unique_columns.values()]

pivot_df

# get sp500 return series
query = """
    SELECT caldt, sprtrn
    FROM crsp.dsp500
    WHERE caldt >= '2017-08-01'
"""

# Execute the query
sp500_returns = conn.raw_sql(query)
sp500_returns.rename(columns={'caldt': 'date', 'sprtrn': 'sp500_returns'}, inplace=True)
sp500_returns['date'] = pd.to_datetime(sp500_returns['date'])
sp500_returns.set_index('date', inplace=True)
print(sp500_returns.head())

return_df_clean = pivot_df

from sklearn.linear_model import LinearRegression
rolling_window = 90
beta_matrix = pd.DataFrame(index=return_df_clean.index, columns=return_df_clean.columns)

for ticker in return_df_clean.columns:
    stock_returns = return_df_clean[ticker]
    market_returns = sp500_returns['sp500_returns']

    for i in range(rolling_window, len(stock_returns)):
        y = stock_returns.iloc[i - rolling_window:i].values.reshape(-1, 1)
        X = market_returns.iloc[i - rolling_window:i].values.reshape(-1, 1)
        # Check for NaNs in the window and skip if present
        if not np.isnan(y).any() and not np.isnan(X).any():
            model = LinearRegression().fit(X, y)
            beta_matrix.iloc[i, beta_matrix.columns.get_loc(ticker)] = model.coef_[0][0]
print(beta_matrix.tail())

excess_return_matrix = pd.DataFrame(index=return_df_clean.index, columns=return_df_clean.columns)
for ticker in return_df_clean.columns:
    stock_returns = return_df_clean[ticker]
    betas = beta_matrix[ticker]
    excess_return_matrix[ticker] = stock_returns - (betas * sp500_returns['sp500_returns'])
print(excess_return_matrix.tail())

excess_return = excess_return_matrix.iloc[103:,]

excess_return.to_csv('excess_return_matrix.csv', index=True)

"""# Phase Two Download"""

sp500_filtered

missing = [sp for sp in sp500_tick if sp not in sp500_filtered['ticker'].values]
len(missing)

# 79 missing but it is allright as they are not really in the period
sp500_comp[sp500_comp['Ticker'] == 'CPGX']

df = excess_return_matrix.copy()
matching_columns = [col for col in df.columns if '_' in col and not any(char.isdigit() for char in col.split('_', 1)[1])]

matching_columns

ticker_cik_dict = {}

for info in sp500_filtered['info']:
    parts = info.split('_')
    if len(parts) == 2 and parts[1].isdigit():
        ticker_cik_dict[parts[0]] = int(parts[1])

len(list(ticker_cik_dict.keys()))

info_table = sp500_df.copy()

info_table['Date'] = pd.to_datetime(info_table['Date'])
filtered_info_table = info_table[info_table['Date'] > pd.Timestamp('2018-01-01')]
info_table_sorted = filtered_info_table.sort_values(by='Date')
info_table_sorted.reset_index(drop=True, inplace=True)

info_table_sorted['cik'] = info_table_sorted['Ticker'].apply(lambda x: ticker_cik_dict.get(x, 0))

info_table_sorted

Q42018 = info_table_sorted[(info_table_sorted['Date'] < pd.Timestamp('2019-01-31'))]
Q42018_cik = Q42018['cik']
Q42018_cik.to_csv('Q42018.txt', sep='\t', index=False)

years = [2018, 2019, 2020, 2021, 2022, 2023]
quarters = {
    'Q1': ('01-31', '04-30'),
    'Q2': ('04-30', '07-31'),
    'Q3': ('07-31', '10-31'),
    'Q4': ('10-31', '12-31')
}

for year in years:
    for quarter, (start_date, end_date) in quarters.items():
        filtered_df = info_table_sorted[
            (info_table_sorted['Date'] > pd.Timestamp(f'{year}-{start_date}')) &
            (info_table_sorted['Date'] <= pd.Timestamp(f'{year}-{end_date}')) &
            (info_table_sorted['cik'] != 0)
        ]

        filtered_cik = filtered_df['cik']
        print(len(filtered_cik))
        filename = f'{quarter}{year}.txt'
        filtered_cik.to_csv(filename, sep='\t', index=False, header=False)

        print(f"Saved {filename}")

"""# Phase Three Construct"""

"""
Program to provide generic parsing for all files in user-specified directory.
The program assumes the input files have been scrubbed,
  i.e., HTML, ASCII-encoded binary, and any other embedded document structures that are not
  intended to be analyzed have been deleted from the file.

Dependencies:
    Python:  Load_MasterDictionary.py
    Data:    LoughranMcDonald_MasterDictionary_XXXX.csv

The program outputs:
   1.  File name
   2.  File size (in bytes)
   3.  Number of words (based on LM_MasterDictionary
   4.  Proportion of positive words (use with care - see LM, JAR 2016)
   5.  Proportion of negative words
   6.  Proportion of uncertainty words
   7.  Proportion of litigious words
   8.  Proportion of modal-weak words
   9.  Proportion of modal-moderate words
  10.  Proportion of modal-strong words
  11.  Proportion of constraining words (see Bodnaruk, Loughran and McDonald, JFQA 2015)
  12.  Number of alphanumeric characters (a-z, A-Z)
  13.  Number of digits (0-9)
  14.  Number of numbers (collections of digits)
  15.  Average number of syllables
  16.  Average word length
  17.  Vocabulary (see Loughran-McDonald, JF, 2015)

  ND-SRAF
  McDonald 2016/06 : updated 2018/03
"""

import csv
import glob
import re
import json
import string
import sys
import time
import Load_MasterDictionary as LM
import numpy as np
import pandas as pd

# User defined directory for files to be parsed
TARGET_FILES = r'E:/ALLEXTRACTED_FILINGS/*.*'

# User defined file pointer to LM dictionary
MASTER_DICTIONARY_FILE = r'C:/Users/26491/LoughranMcDonald_MasterDictionary_2018.csv'

# User defined output file
MATRIX_FILE = r'E:/ALLEXTRACTED_FILINGS/Matrix.csv'

# User defined output file
PROPORTION_FILE = r'E:/ALLEXTRACTED_FILINGS/Proportion.csv'

# User defined output file
# OUTPUT_FILE = r'/E:/ALLEXTRACTED_FILINGS/Parser.csv'

# Setup output
OUTPUT_FIELDS = ['file name,', 'file size,', 'number of words,', '% positive,', '% negative,',
                 '% uncertainty,', '% litigious,', '% modal-weak,', '% modal moderate,',
                 '% modal strong,', '% constraining,', '# of alphabetic,', '# of digits,',
                 '# of numbers,', 'avg # of syllables per word,', 'average word length,', 'vocabulary']

lm_dictionary = LM.load_masterdictionary(MASTER_DICTIONARY_FILE, True)

# Assuming lm_dictionary contains all the negative words from the master dictionary
negative_words = [word for word in lm_dictionary if lm_dictionary[word].negative]

def get_data(doc):

    vdictionary = {}
    _odata = [0] * 17
    total_syllables = 0
    word_length = 0

    tokens = re.findall('\w+', doc)  # Note that \w+ splits hyphenated words
    for token in tokens:
        if not token.isdigit() and len(token) > 1 and token in lm_dictionary:
            _odata[2] += 1  # word count
            word_length += len(token)
            if token not in vdictionary:
                vdictionary[token] = 1
            if lm_dictionary[token].positive: _odata[3] += 1
            if lm_dictionary[token].negative: _odata[4] += 1
            if lm_dictionary[token].uncertainty: _odata[5] += 1
            if lm_dictionary[token].litigious: _odata[6] += 1
            if lm_dictionary[token].weak_modal: _odata[7] += 1
            if lm_dictionary[token].moderate_modal: _odata[8] += 1
            if lm_dictionary[token].strong_modal: _odata[9] += 1
            if lm_dictionary[token].constraining: _odata[10] += 1
            total_syllables += lm_dictionary[token].syllables

    _odata[11] = len(re.findall('[A-Z]', doc))
    _odata[12] = len(re.findall('[0-9]', doc))
    # drop punctuation within numbers for number count
    doc = re.sub('(?!=[0-9])(\.|,)(?=[0-9])', '', doc)
    doc = doc.translate(str.maketrans(string.punctuation, " " * len(string.punctuation)))
    _odata[13] = len(re.findall(r'\b[-+\(]?[$€£]?[-+(]?\d+\)?\b', doc))
    _odata[14] = total_syllables / _odata[2]
    _odata[15] = word_length / _odata[2]
    _odata[16] = len(vdictionary)

    # Convert counts to %
    for i in range(3, 10 + 1):
        _odata[i] = (_odata[i] / _odata[2]) * 100
    # Vocabulary

    return _odata

def get_proportion(tf_matrix, idf_matrix, doc_length_matrix):
    """
    Calculate the modified TF-IDF proportions and weights for each document based on the provided formula.
    """
    num_docs = tf_matrix.shape[0]  # Total number of documents (N)

    # Calculate document frequency (DF) for each word
    df_vector = np.sum(idf_matrix, axis=0)  # How many documents each word appears in

    # Calculate IDF for each word
    idf_vector = np.log((num_docs) / (df_vector + 1))  # IDF with smoothing (add 1 to avoid division by 0)

    # Calculate the average word count (a_j) for each document
    avg_word_count = np.mean(doc_length_matrix)  # Average word count per document

    # Initialize the adjusted TF-IDF matrix
    tfidf_matrix = np.zeros_like(tf_matrix)

    # Apply the adjusted TF formula and calculate the final TF-IDF
    for doc_idx in range(num_docs):
        for word_idx in range(tf_matrix.shape[1]):
            tf_ij = tf_matrix[doc_idx, word_idx]
            if tf_ij > 0:  # Only apply TF-IDF if tf_ij >= 1
                a_j = avg_word_count  # Use the average word count of document j
                # Adjusted TF: (1 + log(tf_ij)) / (1 + log(a_j))
                adjusted_tf = (1 + np.log(tf_ij)) / (1 + np.log(a_j))
                # Final TF-IDF: adjusted TF * IDF
                tfidf_matrix[doc_idx, word_idx] = adjusted_tf * idf_vector[word_idx]
            else:
                # If tf_ij == 0, set the weight to 0 as per the formula
                tfidf_matrix[doc_idx, word_idx] = 0

    # Sum of TF-IDF scores for each document
    tfidf_sums = np.sum(tfidf_matrix, axis=1)

    # Calculate proportion weights by dividing the TF-IDF sum by the total number of words in the document
    proportion_weights = tfidf_sums / doc_length_matrix.flatten()

    return tfidf_matrix, proportion_weights

def main():
    # Initialize matrices
    file_list = glob.glob(TARGET_FILES)
    num_docs = len(file_list)
    num_words = len(negative_words)

    # Initialize TF, IDF, and doc_length matrices
    tf_matrix = np.zeros((num_docs, num_words))
    idf_matrix = np.zeros((num_docs, num_words))
    doc_length_matrix = np.zeros((num_docs, 1))

    # Create lists to store 'filing_time' and 'cik' for each document
    filling_time_list = []
    cik_list = []

    # Process each document and populate matrices
    for doc_idx, file in enumerate(file_list):
        print(f"Processing file: {file}")
        try:
            with open(file, 'r', encoding='UTF-8', errors='ignore') as f_in:
                json_data = json.load(f_in)
        except json.JSONDecodeError:
            print(f"Error: Could not parse JSON in the file {file}. Skipping this file.")
            continue

        # Extract text and metadata from the JSON
        doc = json_data.get('full_text', '').upper()  # Get the document text
        filing_date = json_data.get('filing_date', '')
        cik = json_data.get('cik', '')

        # Append filing date and cik to lists
        filling_time_list.append(filing_date)
        cik_list.append(cik)

        # Use the get_data function to extract data for the current document
        output_data = get_data(doc)

        # Store document length (number of words)
        doc_length_matrix[doc_idx] = output_data[2]

        # Tokenize the document text
        tokens = re.findall(r'\w+', doc)

        # Populate TF and IDF matrices
        for token in tokens:
            if token in negative_words:
                word_idx = negative_words.index(token)
                tf_matrix[doc_idx, word_idx] += 1  # Increment term frequency
                idf_matrix[doc_idx, word_idx] = 1  # Mark presence of word in the document

    # Calculate proportions using the get_proportion function
    tfidf_matrix, proportion_weights = get_proportion(tf_matrix, idf_matrix, doc_length_matrix)

    # Write TF-IDF information to MATRIX_OUTPUT_FILE
    with open(MATRIX_FILE, 'w', newline='') as f_matrix:
        matrix_writer = csv.writer(f_matrix, lineterminator='\n')

        # Write header (document name + each negative word)
        header = ['file name'] + negative_words
        matrix_writer.writerow(header)

        # Write each document's TF-IDF scores
        for doc_idx, file in enumerate(file_list):
            row = [file] + list(tfidf_matrix[doc_idx])
            matrix_writer.writerow(row)

    # Write the proportion weights to PROPORTION_OUTPUT_FILE
    with open(PROPORTION_FILE, 'w', newline='') as f_proportion:
        proportion_writer = csv.writer(f_proportion, lineterminator='\n')

        # Write header
        proportion_writer.writerow(['file name', 'proportion_weight', 'filing_time', 'cik'])

        # Write each document's proportion weight
        for doc_idx, file in enumerate(file_list):
            if doc_idx < len(filling_time_list):  # Ensure we're within bounds for valid entries
                proportion_writer.writerow([file, proportion_weights[doc_idx], filling_time_list[doc_idx], cik_list[doc_idx]])

if __name__ == '__main__':
    import time
    print('\n' + time.strftime('%c') + '\nGeneric_Parser.py\n')
    main()
    print('\n' + time.strftime('%c') + '\nNormal termination.')

tf_idf = pd.read_csv("Matrix.csv")
tf_idf

term_weight = pd.read_csv('Proportion.csv')
term_weight

df = term_weight.copy()
df['quantile'] = pd.qcut(df['proportion_weight'], 5, labels=False) + 1
df.drop(['proportion_weight'], axis=1, inplace=True)
df

# Example demonstration
data = {
    'AAPL': [0.01, 0.02, -0.01, 0.03, -0.02, 0.01, np.nan, 0.04],
    'MSFT': [0.02, np.nan, 0.01, -0.02, 0.03, 0.01, 0.02, -0.01],
    'GOOG': [0.03, 0.01, -0.02, np.nan, 0.01, 0.02, 0.03, 0.00]
}

dfs = pd.DataFrame(data, index=pd.date_range(start='2023-01-01', periods=8))
df_filled = dfs.fillna(0)
cumulative_returns_3d = (df_filled + 1).rolling(window=3).apply(lambda x: x.prod(), raw=True) - 1
next_3_day_cumulative_returns = cumulative_returns_3d.shift(-3)

next_3_day_cumulative_returns

cumulative_returns_3d.shift(-3)

[0.02, -0.01, 0.03]

(1+0.02)*(1-0.01)*(1+0.03) - 1

matching_return = excess_return.fillna(0)
matching_return = (matching_return + 1).rolling(window=3).apply(lambda x: x.prod(), raw=True) - 1
matching_return = matching_return.shift(-3)

df

matching_return.columns = [
    int(col.split('_')[1]) if '_' in col and col.split('_')[1].isdigit() else col.split('_')[1]
    for col in matching_return.columns
]

matching_return

df

def get_closest_trade_date(date, trade_dates):
    future_dates = trade_dates[trade_dates >= date]
    if not future_dates.empty:
        return future_dates[0]
    return None
trade_dates = matching_return.index

df['filing_time'] = pd.to_datetime(df['filing_time'], format='mixed', dayfirst=False, errors='coerce')

df['return'] = df.apply(
    lambda row: matching_return.loc[get_closest_trade_date(row['filing_time'], trade_dates), row['cik']]
    if (get_closest_trade_date(row['filing_time'], trade_dates) is not None and row['cik'] in matching_return.columns)
    else None,
    axis=1
)

df

import matplotlib.pyplot as plt
# Convert the 'return' column to numeric, forcing non-numeric values to NaN
df['return'] = pd.to_numeric(df['return'], errors='coerce')
median_returns = df.groupby('quantile')['return'].mean()
plt.figure(figsize=(10, 6))
plt.plot(median_returns.index, median_returns.values * 100 * 100, marker='o', linestyle='-', color='black')
plt.title('Mean Filing Period Excess Return by Quantile')
plt.xlabel('Quantile (based on proportion of negative words)')
plt.ylabel('Mean Filing Period Excess Return in bps')
plt.xticks(median_returns.index, ['Low', '2', '3', '4', 'High'])
plt.gca().set_facecolor('#FFC0CB')
plt.grid(True, linestyle='--', linewidth=0.7, color='gray')
plt.show()

set(df['quantile'])

median_returns

df[df['quantile'] == 1]

df[df['quantile'] == 4]

"""# Conclusion

It appears that the "McDonald's effect" does not persist based on our findings, and this could be attributed to several factors.

Firstly, the paper indicates that companies have become less inclined to use negative language, possibly due to an increased awareness of how public perception and investor reactions are shaped by sentiment analysis. This shift towards more neutral or positive language could diminish the impact of traditionally negative indicators, leading to reduced predictive power in our analysis.

Secondly, it's important to note that we are only analyzing 10-Q filings, which provide a retrospective view of a company's financial health rather than forward-looking performance forecasts. As 10-Qs are intended primarily to update investors on past performance and current risks, they may not fully capture the optimistic or pessimistic outlook that could directly influence future stock returns.

Lastly, the dictionary we used for sentiment analysis is from 2018, which may limit its effectiveness in capturing evolving language trends. Corporate communication styles and the terminology used in financial reports have likely changed over time, leading to discrepancies between the dictionary's vocabulary and the language currently in use. An outdated dictionary might fail to accurately classify sentiment, thereby reducing the reliability of our findings.

To address these challenges, we may consider updating our sentiment dictionary, incorporating more recent language trends, and expanding the analysis to include other documents such as earnings forecasts or conference call transcripts that contain forward-looking statements, providing a more holistic view of sentiment in relation to company performance.
"""